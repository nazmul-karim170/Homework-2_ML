{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework_2_ML",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "c3WBFqgW6Pmy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**First, We import the necessary python libraries.**"
      ]
    },
    {
      "metadata": {
        "id": "lifCLGBjTD0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adagrad, Adam\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Activation, Flatten, BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import KFold\n",
        "from keras.models import model_from_json\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZTAOQfA6dVB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next, We load the CIFAR10 dataset. It will be downloaded first which might take couple of seconds.**"
      ]
    },
    {
      "metadata": {
        "id": "WRlyXiMp3E0V",
        "colab_type": "code",
        "outputId": "e7349ebc-16b1-4bac-ac00-fd0719f84f40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "## Load the CIFAR10 dataset  \n",
        "(train_images_1, train_labels_1),(test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 58s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1DW9hN8O4Nme",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**After that, we need to create the validation set using 20% of the training samples**"
      ]
    },
    {
      "metadata": {
        "id": "MMGjVvDF3Lhb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Get the training set length\n",
        "training_len = np.shape(train_images_1)[0]\n",
        "validation_len = np.int32(0.2 * training_len)\n",
        "\n",
        "## Get the indices for training and validation split\n",
        "indices = np.random.permutation(len(train_images_1))\n",
        "val_indices = indices[0:validation_len]\n",
        "train_indices = indices[validation_len:]\n",
        "\n",
        "## Separate the validation set from training dataset\n",
        "val_images, val_labels = train_images_1[val_indices], train_labels_1[val_indices]\n",
        "train_images, train_labels = train_images_1[train_indices], train_labels_1[train_indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GT3Yflr44eV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**We can normalize the train, test and validation dataset. The labels are also set for categorical cross entropy loss**"
      ]
    },
    {
      "metadata": {
        "id": "6bPFsQps3erf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Normalizing the train, test and validation sets\n",
        "X_train = (train_images/255).astype('float32')\n",
        "y_train = np_utils.to_categorical(train_labels, 10)\n",
        "\n",
        "X_val = (val_images/255).astype('float32')\n",
        "y_val = np_utils.to_categorical(val_labels, 10)\n",
        "\n",
        "X_test = (test_images/255).astype('float32')\n",
        "y_test = np_utils.to_categorical(test_labels, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DkDyBsN09pAV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Addidtional information for training**"
      ]
    },
    {
      "metadata": {
        "id": "A7lXasGU9vfg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Keras Parameters\n",
        "batch_size = 64\n",
        "nb_classes = 10\n",
        "nb_epochs = 50\n",
        "\n",
        "## Weight deacy for kernel regularizer\n",
        "weight_decay = 1e-4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "boGRNXuV30TA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next, We create the model with Keras. We have used 6 convolution layers, one hidden FC layer and one output layer. **"
      ]
    },
    {
      "metadata": {
        "id": "BLmlExFS3jxh",
        "colab_type": "code",
        "outputId": "7fa4ad0f-5d21-4c22-977d-41f87f05c38e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        }
      },
      "cell_type": "code",
      "source": [
        "## Build the CNN Model with keras\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), strides= (1,1), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(256, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        " \n",
        "  \n",
        "## Add Fully Connected Layer here\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation= 'relu', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_37 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_38 (Conv2D)           (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_39 (Conv2D)           (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_27 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_42 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_28 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,118,778\n",
            "Trainable params: 2,118,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DJko037R45rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next step is for training and validation. I have used ADAgrad Optimizer here. This Section  has been simulated without Dropout.**"
      ]
    },
    {
      "metadata": {
        "id": "EivLMc8c5MC6",
        "colab_type": "code",
        "outputId": "b56ff26b-1a0f-48d7-80fb-2c770229154a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1872
        }
      },
      "cell_type": "code",
      "source": [
        "## Initializing Adagrad Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adagrad',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "history = model.fit(X_train, y_train, batch_size= batch_size, epochs= nb_epochs,\n",
        "                    verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "## Create the saving directory for different configured trained model\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "## Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model_without_dropout.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "\n",
        "## Serialize weights to HDF5\n",
        "model_for_weight = 'Keras_Model_Without Dropout.h5'\n",
        "model_path_1 = os.path.join(save_dir, model_for_weight) \n",
        "model.save_weights(model_path_1)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "40000/40000 [==============================] - 28s 699us/step - loss: 14.6151 - acc: 0.0995 - val_loss: 14.5706 - val_acc: 0.1014\n",
            "Epoch 2/50\n",
            "40000/40000 [==============================] - 25s 620us/step - loss: 8.7872 - acc: 0.1250 - val_loss: 2.0672 - val_acc: 0.2457\n",
            "Epoch 3/50\n",
            "40000/40000 [==============================] - 25s 620us/step - loss: 1.6972 - acc: 0.3989 - val_loss: 1.4923 - val_acc: 0.4690\n",
            "Epoch 4/50\n",
            "40000/40000 [==============================] - 25s 620us/step - loss: 1.3487 - acc: 0.5389 - val_loss: 1.2067 - val_acc: 0.5936\n",
            "Epoch 5/50\n",
            "40000/40000 [==============================] - 25s 621us/step - loss: 1.1225 - acc: 0.6225 - val_loss: 1.1271 - val_acc: 0.6282\n",
            "Epoch 6/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.9298 - acc: 0.6964 - val_loss: 0.9181 - val_acc: 0.7049\n",
            "Epoch 7/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.7988 - acc: 0.7468 - val_loss: 0.8911 - val_acc: 0.7169\n",
            "Epoch 8/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.6910 - acc: 0.7864 - val_loss: 0.8178 - val_acc: 0.7470\n",
            "Epoch 9/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.5981 - acc: 0.8202 - val_loss: 0.8345 - val_acc: 0.7417\n",
            "Epoch 10/50\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.5148 - acc: 0.8505 - val_loss: 0.8080 - val_acc: 0.7582\n",
            "Epoch 11/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.4358 - acc: 0.8800 - val_loss: 0.8219 - val_acc: 0.7597\n",
            "Epoch 12/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.3647 - acc: 0.9062 - val_loss: 0.8605 - val_acc: 0.7628\n",
            "Epoch 13/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.3002 - acc: 0.9298 - val_loss: 0.9253 - val_acc: 0.7562\n",
            "Epoch 14/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.2385 - acc: 0.9533 - val_loss: 0.9692 - val_acc: 0.7671\n",
            "Epoch 15/50\n",
            "40000/40000 [==============================] - 25s 616us/step - loss: 0.1925 - acc: 0.9702 - val_loss: 1.0507 - val_acc: 0.7666\n",
            "Epoch 16/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.1563 - acc: 0.9828 - val_loss: 1.1539 - val_acc: 0.7673\n",
            "Epoch 17/50\n",
            "40000/40000 [==============================] - 25s 618us/step - loss: 0.1326 - acc: 0.9912 - val_loss: 1.2412 - val_acc: 0.7629\n",
            "Epoch 18/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.1137 - acc: 0.9965 - val_loss: 1.2633 - val_acc: 0.7696\n",
            "Epoch 19/50\n",
            "40000/40000 [==============================] - 25s 617us/step - loss: 0.1003 - acc: 0.9995 - val_loss: 1.3634 - val_acc: 0.7659\n",
            "Epoch 20/50\n",
            "40000/40000 [==============================] - 25s 618us/step - loss: 0.0948 - acc: 1.0000 - val_loss: 1.4287 - val_acc: 0.7700\n",
            "Epoch 21/50\n",
            "40000/40000 [==============================] - 25s 618us/step - loss: 0.0922 - acc: 1.0000 - val_loss: 1.4630 - val_acc: 0.7702\n",
            "Epoch 22/50\n",
            "40000/40000 [==============================] - 25s 613us/step - loss: 0.0905 - acc: 1.0000 - val_loss: 1.4940 - val_acc: 0.7694\n",
            "Epoch 23/50\n",
            "40000/40000 [==============================] - 24s 612us/step - loss: 0.0892 - acc: 1.0000 - val_loss: 1.5254 - val_acc: 0.7678\n",
            "Epoch 24/50\n",
            "40000/40000 [==============================] - 25s 613us/step - loss: 0.0880 - acc: 1.0000 - val_loss: 1.5393 - val_acc: 0.7680\n",
            "Epoch 25/50\n",
            "40000/40000 [==============================] - 28s 704us/step - loss: 0.0869 - acc: 1.0000 - val_loss: 1.5508 - val_acc: 0.7681\n",
            "Epoch 26/50\n",
            "40000/40000 [==============================] - 25s 633us/step - loss: 0.0859 - acc: 1.0000 - val_loss: 1.5635 - val_acc: 0.7673\n",
            "Epoch 27/50\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.0849 - acc: 1.0000 - val_loss: 1.5750 - val_acc: 0.7671\n",
            "Epoch 28/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0840 - acc: 1.0000 - val_loss: 1.5846 - val_acc: 0.7667\n",
            "Epoch 29/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0831 - acc: 1.0000 - val_loss: 1.5916 - val_acc: 0.7650\n",
            "Epoch 30/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0823 - acc: 1.0000 - val_loss: 1.5995 - val_acc: 0.7662\n",
            "Epoch 31/50\n",
            "40000/40000 [==============================] - 25s 624us/step - loss: 0.0815 - acc: 1.0000 - val_loss: 1.6053 - val_acc: 0.7663\n",
            "Epoch 32/50\n",
            "40000/40000 [==============================] - 25s 624us/step - loss: 0.0807 - acc: 1.0000 - val_loss: 1.6072 - val_acc: 0.7660\n",
            "Epoch 33/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0799 - acc: 1.0000 - val_loss: 1.6124 - val_acc: 0.7644\n",
            "Epoch 34/50\n",
            "40000/40000 [==============================] - 25s 624us/step - loss: 0.0792 - acc: 1.0000 - val_loss: 1.6216 - val_acc: 0.7649\n",
            "Epoch 35/50\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.0785 - acc: 1.0000 - val_loss: 1.6202 - val_acc: 0.7649\n",
            "Epoch 36/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0778 - acc: 1.0000 - val_loss: 1.6236 - val_acc: 0.7652\n",
            "Epoch 37/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0771 - acc: 1.0000 - val_loss: 1.6226 - val_acc: 0.7637\n",
            "Epoch 38/50\n",
            "40000/40000 [==============================] - 25s 624us/step - loss: 0.0764 - acc: 1.0000 - val_loss: 1.6266 - val_acc: 0.7640\n",
            "Epoch 39/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0758 - acc: 1.0000 - val_loss: 1.6337 - val_acc: 0.7649\n",
            "Epoch 40/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0751 - acc: 1.0000 - val_loss: 1.6353 - val_acc: 0.7646\n",
            "Epoch 41/50\n",
            "40000/40000 [==============================] - 25s 626us/step - loss: 0.0745 - acc: 1.0000 - val_loss: 1.6355 - val_acc: 0.7651\n",
            "Epoch 42/50\n",
            "40000/40000 [==============================] - 25s 626us/step - loss: 0.0739 - acc: 1.0000 - val_loss: 1.6422 - val_acc: 0.7638\n",
            "Epoch 43/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0733 - acc: 1.0000 - val_loss: 1.6464 - val_acc: 0.7626\n",
            "Epoch 44/50\n",
            "40000/40000 [==============================] - 25s 625us/step - loss: 0.0728 - acc: 1.0000 - val_loss: 1.6459 - val_acc: 0.7625\n",
            "Epoch 45/50\n",
            "40000/40000 [==============================] - 25s 626us/step - loss: 0.0722 - acc: 1.0000 - val_loss: 1.6451 - val_acc: 0.7621\n",
            "Epoch 46/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0717 - acc: 1.0000 - val_loss: 1.6478 - val_acc: 0.7656\n",
            "Epoch 47/50\n",
            "40000/40000 [==============================] - 25s 623us/step - loss: 0.0711 - acc: 1.0000 - val_loss: 1.6523 - val_acc: 0.7638\n",
            "Epoch 48/50\n",
            "40000/40000 [==============================] - 25s 624us/step - loss: 0.0706 - acc: 1.0000 - val_loss: 1.6591 - val_acc: 0.7636\n",
            "Epoch 49/50\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 1.6544 - val_acc: 0.7631\n",
            "Epoch 50/50\n",
            "40000/40000 [==============================] - 25s 622us/step - loss: 0.0695 - acc: 1.0000 - val_loss: 1.6615 - val_acc: 0.7638\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3QFNpCPA7ZZQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Second part is for the model with Dropout. We have added this to avoid Overfitting (As we can see from previous architecture, training accuracy gets saturated while validation accuracy is stuck to a certain value. It means that the model is overfitted).**\n"
      ]
    },
    {
      "metadata": {
        "id": "3MIl_D6H7Rsy",
        "colab_type": "code",
        "outputId": "92bbcff2-e2b0-4773-dac3-512dd1f3f586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2617
        }
      },
      "cell_type": "code",
      "source": [
        "## Build the CNN Model with keras\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        "model.add(Dropout(0.2))                                      #adding dropout layer\n",
        "\n",
        " \n",
        "model.add(Conv2D(128, (3,3), strides= (1,1), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.3))                                      #adding dropout layer\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.4))                                      #adding dropout layer\n",
        "\n",
        "model.add(Conv2D(512, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.5))                                      #adding dropout layer\n",
        " \n",
        "  \n",
        "## Add Fully Connected Layer here\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation= 'relu', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "          \n",
        "          \n",
        "## Initializing Adagrad Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adagrad',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "history = model.fit(X_train, y_train, batch_size= batch_size, epochs= nb_epochs,\n",
        "                    verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "## Create the saving directory for different configured trained model\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "## Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model_with_dropout.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "\n",
        "## Serialize weights to HDF5\n",
        "model_for_weight = 'Keras_Model_With Dropout.h5'\n",
        "model_path_2 = os.path.join(save_dir, model_for_weight) \n",
        "model.save_weights(model_path_2)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_43 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_47 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_48 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,118,778\n",
            "Trainable params: 2,118,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "40000/40000 [==============================] - 30s 750us/step - loss: 5.1442 - acc: 0.1740 - val_loss: 1.9099 - val_acc: 0.3606\n",
            "Epoch 2/50\n",
            "40000/40000 [==============================] - 27s 664us/step - loss: 1.7181 - acc: 0.4249 - val_loss: 1.4756 - val_acc: 0.5338\n",
            "Epoch 3/50\n",
            "40000/40000 [==============================] - 27s 665us/step - loss: 1.4347 - acc: 0.5358 - val_loss: 1.2859 - val_acc: 0.6082\n",
            "Epoch 4/50\n",
            "40000/40000 [==============================] - 27s 665us/step - loss: 1.2619 - acc: 0.6015 - val_loss: 1.1316 - val_acc: 0.6535\n",
            "Epoch 5/50\n",
            "40000/40000 [==============================] - 27s 665us/step - loss: 1.1432 - acc: 0.6453 - val_loss: 1.0264 - val_acc: 0.6951\n",
            "Epoch 6/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 1.0550 - acc: 0.6789 - val_loss: 0.9520 - val_acc: 0.7230\n",
            "Epoch 7/50\n",
            "40000/40000 [==============================] - 27s 667us/step - loss: 0.9844 - acc: 0.7079 - val_loss: 0.8914 - val_acc: 0.7445\n",
            "Epoch 8/50\n",
            "40000/40000 [==============================] - 27s 667us/step - loss: 0.9262 - acc: 0.7248 - val_loss: 0.8516 - val_acc: 0.7607\n",
            "Epoch 9/50\n",
            "40000/40000 [==============================] - 27s 663us/step - loss: 0.8824 - acc: 0.7424 - val_loss: 0.8345 - val_acc: 0.7709\n",
            "Epoch 10/50\n",
            "40000/40000 [==============================] - 27s 667us/step - loss: 0.8447 - acc: 0.7548 - val_loss: 0.8173 - val_acc: 0.7704\n",
            "Epoch 11/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.8090 - acc: 0.7683 - val_loss: 0.7900 - val_acc: 0.7822\n",
            "Epoch 12/50\n",
            "40000/40000 [==============================] - 27s 665us/step - loss: 0.7785 - acc: 0.7805 - val_loss: 0.7610 - val_acc: 0.7918\n",
            "Epoch 13/50\n",
            "40000/40000 [==============================] - 27s 664us/step - loss: 0.7533 - acc: 0.7870 - val_loss: 0.7924 - val_acc: 0.7775\n",
            "Epoch 14/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.7287 - acc: 0.7982 - val_loss: 0.7149 - val_acc: 0.8052\n",
            "Epoch 15/50\n",
            "40000/40000 [==============================] - 27s 664us/step - loss: 0.7130 - acc: 0.8029 - val_loss: 0.7329 - val_acc: 0.7997\n",
            "Epoch 16/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.6907 - acc: 0.8120 - val_loss: 0.7218 - val_acc: 0.8056\n",
            "Epoch 17/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.6726 - acc: 0.8175 - val_loss: 0.7060 - val_acc: 0.8085\n",
            "Epoch 18/50\n",
            "40000/40000 [==============================] - 27s 664us/step - loss: 0.6556 - acc: 0.8236 - val_loss: 0.6855 - val_acc: 0.8171\n",
            "Epoch 19/50\n",
            "40000/40000 [==============================] - 27s 665us/step - loss: 0.6392 - acc: 0.8297 - val_loss: 0.6909 - val_acc: 0.8161\n",
            "Epoch 20/50\n",
            "40000/40000 [==============================] - 27s 664us/step - loss: 0.6266 - acc: 0.8345 - val_loss: 0.6836 - val_acc: 0.8157\n",
            "Epoch 21/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.6142 - acc: 0.8379 - val_loss: 0.6747 - val_acc: 0.8225\n",
            "Epoch 22/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.6000 - acc: 0.8420 - val_loss: 0.6520 - val_acc: 0.8304\n",
            "Epoch 23/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.5900 - acc: 0.8473 - val_loss: 0.6588 - val_acc: 0.8288\n",
            "Epoch 24/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.5755 - acc: 0.8528 - val_loss: 0.6689 - val_acc: 0.8238\n",
            "Epoch 25/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.5658 - acc: 0.8560 - val_loss: 0.6541 - val_acc: 0.8316\n",
            "Epoch 26/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.5566 - acc: 0.8588 - val_loss: 0.6384 - val_acc: 0.8365\n",
            "Epoch 27/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.5439 - acc: 0.8637 - val_loss: 0.6464 - val_acc: 0.8354\n",
            "Epoch 28/50\n",
            "40000/40000 [==============================] - 27s 667us/step - loss: 0.5328 - acc: 0.8675 - val_loss: 0.6345 - val_acc: 0.8399\n",
            "Epoch 29/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.5277 - acc: 0.8704 - val_loss: 0.6417 - val_acc: 0.8372\n",
            "Epoch 30/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.5199 - acc: 0.8736 - val_loss: 0.6305 - val_acc: 0.8395\n",
            "Epoch 31/50\n",
            "40000/40000 [==============================] - 27s 671us/step - loss: 0.5071 - acc: 0.8772 - val_loss: 0.6379 - val_acc: 0.8359\n",
            "Epoch 32/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.5035 - acc: 0.8789 - val_loss: 0.6349 - val_acc: 0.8390\n",
            "Epoch 33/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4892 - acc: 0.8833 - val_loss: 0.6330 - val_acc: 0.8385\n",
            "Epoch 34/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.4866 - acc: 0.8847 - val_loss: 0.6384 - val_acc: 0.8388\n",
            "Epoch 35/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4830 - acc: 0.8858 - val_loss: 0.6183 - val_acc: 0.8442\n",
            "Epoch 36/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4739 - acc: 0.8896 - val_loss: 0.6287 - val_acc: 0.8415\n",
            "Epoch 37/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4672 - acc: 0.8910 - val_loss: 0.6211 - val_acc: 0.8457\n",
            "Epoch 38/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4592 - acc: 0.8947 - val_loss: 0.6196 - val_acc: 0.8434\n",
            "Epoch 39/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4584 - acc: 0.8959 - val_loss: 0.6253 - val_acc: 0.8441\n",
            "Epoch 40/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.4511 - acc: 0.8972 - val_loss: 0.6208 - val_acc: 0.8478\n",
            "Epoch 41/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4470 - acc: 0.8991 - val_loss: 0.6205 - val_acc: 0.8425\n",
            "Epoch 42/50\n",
            "40000/40000 [==============================] - 27s 668us/step - loss: 0.4373 - acc: 0.9038 - val_loss: 0.6315 - val_acc: 0.8432\n",
            "Epoch 43/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.4321 - acc: 0.9041 - val_loss: 0.6142 - val_acc: 0.8477\n",
            "Epoch 44/50\n",
            "40000/40000 [==============================] - 27s 666us/step - loss: 0.4292 - acc: 0.9062 - val_loss: 0.6154 - val_acc: 0.8481\n",
            "Epoch 45/50\n",
            "40000/40000 [==============================] - 27s 670us/step - loss: 0.4270 - acc: 0.9066 - val_loss: 0.6059 - val_acc: 0.8490\n",
            "Epoch 46/50\n",
            "40000/40000 [==============================] - 27s 667us/step - loss: 0.4189 - acc: 0.9087 - val_loss: 0.6157 - val_acc: 0.8518\n",
            "Epoch 47/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4188 - acc: 0.9097 - val_loss: 0.6162 - val_acc: 0.8485\n",
            "Epoch 48/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4085 - acc: 0.9113 - val_loss: 0.6199 - val_acc: 0.8467\n",
            "Epoch 49/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4095 - acc: 0.9138 - val_loss: 0.6215 - val_acc: 0.8498\n",
            "Epoch 50/50\n",
            "40000/40000 [==============================] - 27s 669us/step - loss: 0.4099 - acc: 0.9131 - val_loss: 0.6284 - val_acc: 0.8461\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o8VO_k8S75bd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Next we do the Data Augmentation.**"
      ]
    },
    {
      "metadata": {
        "id": "elrRAfgHRvMN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        shear_range=0.15,\n",
        "        zoom_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FbNwgcKA8j71",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** First, we do the training with dropout layer.**"
      ]
    },
    {
      "metadata": {
        "id": "2b-lo7Zo9GD8",
        "colab_type": "code",
        "outputId": "0f627648-52f9-4eda-9657-53fabb2731ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5145
        }
      },
      "cell_type": "code",
      "source": [
        "## Build the CNN Model with keras\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        "model.add(Dropout(0.2))                                      #adding dropout layer\n",
        "\n",
        " \n",
        "model.add(Conv2D(128, (3,3), strides= (1,1), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.3))                                      #adding dropout layer\n",
        "\n",
        "\n",
        "model.add(Conv2D(256, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.4))                                      #adding dropout layer\n",
        "\n",
        "model.add(Conv2D(512, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "model.add(Dropout(0.5))                                      #adding dropout layer\n",
        " \n",
        "  \n",
        "## Add Fully Connected Layer here\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation= 'relu', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Initiate Adagrad/adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "model.fit_generator(datagen.flow(X_train, y_train, batch_size=40),\\\n",
        "                    steps_per_epoch=train_images.shape[0] // batch_size,epochs=120,\\\n",
        "                    verbose=1,validation_data=(X_val,y_val))\n",
        "          \n",
        "## Create the saving directory for different configured trained model\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "## Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model_Data_augmentation_with_dropout.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "\n",
        "## Serialize weights to HDF5\n",
        "model_for_weight = 'Keras_Model_Data_augmentation_With Dropout.h5'\n",
        "model_path_3 = os.path.join(save_dir, model_for_weight) \n",
        "model.save_weights(model_path_3)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_19 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_20 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,118,778\n",
            "Trainable params: 2,118,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/120\n",
            "625/625 [==============================] - 28s 44ms/step - loss: 14.6715 - acc: 0.1012 - val_loss: 14.6996 - val_acc: 0.0980\n",
            "Epoch 2/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 14.6376 - acc: 0.1008 - val_loss: 14.6674 - val_acc: 0.0980\n",
            "Epoch 3/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 14.6409 - acc: 0.0990 - val_loss: 14.6454 - val_acc: 0.0980\n",
            "Epoch 4/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 8.2783 - acc: 0.1318 - val_loss: 2.0644 - val_acc: 0.2611\n",
            "Epoch 5/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.9387 - acc: 0.3146 - val_loss: 1.7105 - val_acc: 0.3980\n",
            "Epoch 6/120\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 1.7280 - acc: 0.4025 - val_loss: 1.5598 - val_acc: 0.4770\n",
            "Epoch 7/120\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 1.6074 - acc: 0.4434 - val_loss: 1.4196 - val_acc: 0.5187\n",
            "Epoch 8/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.5119 - acc: 0.4826 - val_loss: 1.3806 - val_acc: 0.5367\n",
            "Epoch 9/120\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 1.4404 - acc: 0.5134 - val_loss: 1.2841 - val_acc: 0.5754\n",
            "Epoch 10/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.3886 - acc: 0.5294 - val_loss: 1.2632 - val_acc: 0.5778\n",
            "Epoch 11/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 1.3282 - acc: 0.5596 - val_loss: 1.1267 - val_acc: 0.6334\n",
            "Epoch 12/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 1.2811 - acc: 0.5787 - val_loss: 1.2159 - val_acc: 0.6101\n",
            "Epoch 13/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 1.2414 - acc: 0.5910 - val_loss: 1.1216 - val_acc: 0.6366\n",
            "Epoch 14/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 1.2175 - acc: 0.6017 - val_loss: 1.1942 - val_acc: 0.6257\n",
            "Epoch 15/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.1743 - acc: 0.6153 - val_loss: 1.0940 - val_acc: 0.6515\n",
            "Epoch 16/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 1.1561 - acc: 0.6244 - val_loss: 1.0213 - val_acc: 0.6764\n",
            "Epoch 17/120\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 1.1264 - acc: 0.6365 - val_loss: 1.0189 - val_acc: 0.6832\n",
            "Epoch 18/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.1024 - acc: 0.6461 - val_loss: 1.0091 - val_acc: 0.6826\n",
            "Epoch 19/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.0934 - acc: 0.6461 - val_loss: 0.9710 - val_acc: 0.6971\n",
            "Epoch 20/120\n",
            "625/625 [==============================] - 24s 38ms/step - loss: 1.0615 - acc: 0.6599 - val_loss: 0.9327 - val_acc: 0.7081\n",
            "Epoch 21/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.0454 - acc: 0.6677 - val_loss: 0.9786 - val_acc: 0.6921\n",
            "Epoch 22/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.0332 - acc: 0.6721 - val_loss: 0.8702 - val_acc: 0.7303\n",
            "Epoch 23/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 1.0096 - acc: 0.6814 - val_loss: 0.8640 - val_acc: 0.7346\n",
            "Epoch 24/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9970 - acc: 0.6864 - val_loss: 0.9061 - val_acc: 0.7229\n",
            "Epoch 25/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.9966 - acc: 0.6846 - val_loss: 0.8616 - val_acc: 0.7344\n",
            "Epoch 26/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.9658 - acc: 0.6968 - val_loss: 0.8240 - val_acc: 0.7467\n",
            "Epoch 27/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.9679 - acc: 0.6989 - val_loss: 0.8170 - val_acc: 0.7538\n",
            "Epoch 28/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9556 - acc: 0.7025 - val_loss: 0.8153 - val_acc: 0.7522\n",
            "Epoch 29/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.9406 - acc: 0.7069 - val_loss: 0.8089 - val_acc: 0.7491\n",
            "Epoch 30/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9393 - acc: 0.7067 - val_loss: 0.7816 - val_acc: 0.7617\n",
            "Epoch 31/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9215 - acc: 0.7096 - val_loss: 0.7527 - val_acc: 0.7742\n",
            "Epoch 32/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9150 - acc: 0.7186 - val_loss: 0.7963 - val_acc: 0.7584\n",
            "Epoch 33/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.9002 - acc: 0.7228 - val_loss: 0.8260 - val_acc: 0.7511\n",
            "Epoch 34/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8964 - acc: 0.7230 - val_loss: 0.8364 - val_acc: 0.7448\n",
            "Epoch 35/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8964 - acc: 0.7234 - val_loss: 0.7621 - val_acc: 0.7740\n",
            "Epoch 36/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8822 - acc: 0.7266 - val_loss: 0.7319 - val_acc: 0.7817\n",
            "Epoch 37/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8797 - acc: 0.7302 - val_loss: 0.7585 - val_acc: 0.7715\n",
            "Epoch 38/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.8805 - acc: 0.7283 - val_loss: 0.8003 - val_acc: 0.7586\n",
            "Epoch 39/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.8613 - acc: 0.7335 - val_loss: 0.7616 - val_acc: 0.7715\n",
            "Epoch 40/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.8617 - acc: 0.7332 - val_loss: 0.7834 - val_acc: 0.7625\n",
            "Epoch 41/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8564 - acc: 0.7361 - val_loss: 0.7533 - val_acc: 0.7716\n",
            "Epoch 42/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8493 - acc: 0.7398 - val_loss: 0.7288 - val_acc: 0.7821\n",
            "Epoch 43/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8458 - acc: 0.7405 - val_loss: 0.7618 - val_acc: 0.7709\n",
            "Epoch 44/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8449 - acc: 0.7383 - val_loss: 0.7472 - val_acc: 0.7767\n",
            "Epoch 45/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8301 - acc: 0.7455 - val_loss: 0.7729 - val_acc: 0.7713\n",
            "Epoch 46/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.8315 - acc: 0.7479 - val_loss: 0.7370 - val_acc: 0.7816\n",
            "Epoch 47/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.8347 - acc: 0.7456 - val_loss: 0.7093 - val_acc: 0.7902\n",
            "Epoch 48/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8265 - acc: 0.7497 - val_loss: 0.7200 - val_acc: 0.7870\n",
            "Epoch 49/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8168 - acc: 0.7513 - val_loss: 0.7100 - val_acc: 0.7923\n",
            "Epoch 50/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8230 - acc: 0.7517 - val_loss: 0.7173 - val_acc: 0.7879\n",
            "Epoch 51/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8041 - acc: 0.7534 - val_loss: 0.6829 - val_acc: 0.7981\n",
            "Epoch 52/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8124 - acc: 0.7546 - val_loss: 0.6711 - val_acc: 0.8032\n",
            "Epoch 53/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7902 - acc: 0.7621 - val_loss: 0.7370 - val_acc: 0.7875\n",
            "Epoch 54/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7981 - acc: 0.7586 - val_loss: 0.6935 - val_acc: 0.7958\n",
            "Epoch 55/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7955 - acc: 0.7571 - val_loss: 0.6966 - val_acc: 0.7974\n",
            "Epoch 56/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.8022 - acc: 0.7576 - val_loss: 0.7087 - val_acc: 0.7915\n",
            "Epoch 57/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7889 - acc: 0.7631 - val_loss: 0.7064 - val_acc: 0.7923\n",
            "Epoch 58/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7892 - acc: 0.7592 - val_loss: 0.6725 - val_acc: 0.8027\n",
            "Epoch 59/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7894 - acc: 0.7617 - val_loss: 0.6647 - val_acc: 0.8075\n",
            "Epoch 60/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7719 - acc: 0.7674 - val_loss: 0.6896 - val_acc: 0.7978\n",
            "Epoch 61/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7734 - acc: 0.7690 - val_loss: 0.7197 - val_acc: 0.7892\n",
            "Epoch 62/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7734 - acc: 0.7660 - val_loss: 0.6506 - val_acc: 0.8101\n",
            "Epoch 63/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.7695 - acc: 0.7682 - val_loss: 0.6564 - val_acc: 0.8090\n",
            "Epoch 64/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.7645 - acc: 0.7679 - val_loss: 0.6703 - val_acc: 0.8063\n",
            "Epoch 65/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7648 - acc: 0.7723 - val_loss: 0.6469 - val_acc: 0.8109\n",
            "Epoch 66/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7600 - acc: 0.7728 - val_loss: 0.7006 - val_acc: 0.7944\n",
            "Epoch 67/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7614 - acc: 0.7692 - val_loss: 0.6456 - val_acc: 0.8128\n",
            "Epoch 68/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7464 - acc: 0.7761 - val_loss: 0.6699 - val_acc: 0.8053\n",
            "Epoch 69/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7578 - acc: 0.7729 - val_loss: 0.6892 - val_acc: 0.7986\n",
            "Epoch 70/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7507 - acc: 0.7788 - val_loss: 0.6693 - val_acc: 0.8074\n",
            "Epoch 71/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7382 - acc: 0.7789 - val_loss: 0.6948 - val_acc: 0.8008\n",
            "Epoch 72/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7471 - acc: 0.7780 - val_loss: 0.6646 - val_acc: 0.8072\n",
            "Epoch 73/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7395 - acc: 0.7798 - val_loss: 0.7047 - val_acc: 0.7938\n",
            "Epoch 74/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7463 - acc: 0.7778 - val_loss: 0.6723 - val_acc: 0.8066\n",
            "Epoch 75/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7313 - acc: 0.7813 - val_loss: 0.6228 - val_acc: 0.8192\n",
            "Epoch 76/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7376 - acc: 0.7786 - val_loss: 0.6218 - val_acc: 0.8232\n",
            "Epoch 77/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7326 - acc: 0.7814 - val_loss: 0.6665 - val_acc: 0.8086\n",
            "Epoch 78/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7270 - acc: 0.7818 - val_loss: 0.6550 - val_acc: 0.8114\n",
            "Epoch 79/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7282 - acc: 0.7835 - val_loss: 0.6053 - val_acc: 0.8286\n",
            "Epoch 80/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7265 - acc: 0.7850 - val_loss: 0.6374 - val_acc: 0.8183\n",
            "Epoch 81/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7221 - acc: 0.7855 - val_loss: 0.6214 - val_acc: 0.8246\n",
            "Epoch 82/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7238 - acc: 0.7862 - val_loss: 0.6200 - val_acc: 0.8227\n",
            "Epoch 83/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7134 - acc: 0.7891 - val_loss: 0.6334 - val_acc: 0.8202\n",
            "Epoch 84/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7238 - acc: 0.7857 - val_loss: 0.6484 - val_acc: 0.8137\n",
            "Epoch 85/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7238 - acc: 0.7844 - val_loss: 0.6070 - val_acc: 0.8269\n",
            "Epoch 86/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7143 - acc: 0.7886 - val_loss: 0.6252 - val_acc: 0.8230\n",
            "Epoch 87/120\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.7184 - acc: 0.7884 - val_loss: 0.6274 - val_acc: 0.8213\n",
            "Epoch 88/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7086 - acc: 0.7943 - val_loss: 0.6275 - val_acc: 0.8201\n",
            "Epoch 89/120\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.7165 - acc: 0.7894 - val_loss: 0.6108 - val_acc: 0.8249\n",
            "Epoch 90/120\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.7005 - acc: 0.7958 - val_loss: 0.5939 - val_acc: 0.8318\n",
            "Epoch 91/120\n",
            "625/625 [==============================] - 26s 41ms/step - loss: 0.7004 - acc: 0.7942 - val_loss: 0.5838 - val_acc: 0.8380\n",
            "Epoch 92/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.7079 - acc: 0.7904 - val_loss: 0.6223 - val_acc: 0.8237\n",
            "Epoch 93/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6998 - acc: 0.7919 - val_loss: 0.6407 - val_acc: 0.8162\n",
            "Epoch 94/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.7008 - acc: 0.7954 - val_loss: 0.5919 - val_acc: 0.8331\n",
            "Epoch 95/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6921 - acc: 0.7949 - val_loss: 0.6125 - val_acc: 0.8259\n",
            "Epoch 96/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6972 - acc: 0.7990 - val_loss: 0.6019 - val_acc: 0.8301\n",
            "Epoch 97/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6933 - acc: 0.7978 - val_loss: 0.6136 - val_acc: 0.8271\n",
            "Epoch 98/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6940 - acc: 0.7972 - val_loss: 0.6685 - val_acc: 0.8089\n",
            "Epoch 99/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6912 - acc: 0.7951 - val_loss: 0.6101 - val_acc: 0.8299\n",
            "Epoch 100/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6838 - acc: 0.7984 - val_loss: 0.6133 - val_acc: 0.8271\n",
            "Epoch 101/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6880 - acc: 0.7981 - val_loss: 0.6003 - val_acc: 0.8317\n",
            "Epoch 102/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6907 - acc: 0.7945 - val_loss: 0.6075 - val_acc: 0.8297\n",
            "Epoch 103/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6792 - acc: 0.8015 - val_loss: 0.5792 - val_acc: 0.8391\n",
            "Epoch 104/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6823 - acc: 0.8008 - val_loss: 0.6009 - val_acc: 0.8294\n",
            "Epoch 105/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6811 - acc: 0.8012 - val_loss: 0.6063 - val_acc: 0.8313\n",
            "Epoch 106/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6775 - acc: 0.8012 - val_loss: 0.5921 - val_acc: 0.8340\n",
            "Epoch 107/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6798 - acc: 0.8003 - val_loss: 0.5857 - val_acc: 0.8336\n",
            "Epoch 108/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6696 - acc: 0.8054 - val_loss: 0.6283 - val_acc: 0.8205\n",
            "Epoch 109/120\n",
            "625/625 [==============================] - 24s 39ms/step - loss: 0.6769 - acc: 0.8031 - val_loss: 0.5985 - val_acc: 0.8323\n",
            "Epoch 110/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6647 - acc: 0.8056 - val_loss: 0.6021 - val_acc: 0.8306\n",
            "Epoch 111/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6653 - acc: 0.8044 - val_loss: 0.5831 - val_acc: 0.8396\n",
            "Epoch 112/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6750 - acc: 0.8020 - val_loss: 0.5699 - val_acc: 0.8408\n",
            "Epoch 113/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6643 - acc: 0.8070 - val_loss: 0.5998 - val_acc: 0.8320\n",
            "Epoch 114/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6707 - acc: 0.8041 - val_loss: 0.5774 - val_acc: 0.8396\n",
            "Epoch 115/120\n",
            "625/625 [==============================] - 25s 39ms/step - loss: 0.6687 - acc: 0.8060 - val_loss: 0.5912 - val_acc: 0.8336\n",
            "Epoch 116/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6635 - acc: 0.8089 - val_loss: 0.6195 - val_acc: 0.8237\n",
            "Epoch 117/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6707 - acc: 0.8026 - val_loss: 0.5662 - val_acc: 0.8435\n",
            "Epoch 118/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6645 - acc: 0.8064 - val_loss: 0.5715 - val_acc: 0.8401\n",
            "Epoch 119/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6644 - acc: 0.8068 - val_loss: 0.5752 - val_acc: 0.8412\n",
            "Epoch 120/120\n",
            "625/625 [==============================] - 25s 40ms/step - loss: 0.6651 - acc: 0.8054 - val_loss: 0.5829 - val_acc: 0.8357\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DIFCG8t0-MA0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Then We do it without the Dropout. **"
      ]
    },
    {
      "metadata": {
        "id": "bYnpJL8F-KnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4999
        },
        "outputId": "0673ea17-e79c-4405-d194-e0d4afb6e267"
      },
      "cell_type": "code",
      "source": [
        "## Build the CNN Model with keras\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), strides = (1,1), padding='same', activation ='relu', input_shape =(32,32,3),kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Conv2D(48, (3,3), strides = (1,1), padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), strides= (1,1),padding='same', activation='relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding= 'same'))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), strides= (1,1), padding='same', activation = 'relu',kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(256, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        "\n",
        "model.add(Conv2D(512, (3,3), strides= (1,1), padding='same', activation = 'relu', kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
        " \n",
        "  \n",
        "## Add Fully Connected Layer here\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation= 'relu', kernel_regularizer= regularizers.l2(weight_decay)))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Initiate Adagrad/adam optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "## Train the model and get the training and validation loss\n",
        "model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=train_images.shape[0] // batch_size,epochs=120,\\\n",
        "                    verbose=1,validation_data=(X_val,y_val))\n",
        "          \n",
        "## Create the saving directory for different configured trained model\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "## Serialize model to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"model_Data_augmentation_without_dropout.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "\n",
        "## Serialize weights to HDF5\n",
        "model_for_weight = 'Keras_Model_Data_augmentation_Without Dropout.h5'\n",
        "model_path_4 = os.path.join(save_dir, model_for_weight) \n",
        "model.save_weights(model_path_4)\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_31 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 32, 32, 48)        13872     \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 32, 32, 64)        27712     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_21 (MaxPooling (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 16, 16, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_22 (MaxPooling (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 8, 8, 256)         295168    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_23 (MaxPooling (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_36 (Conv2D)           (None, 4, 4, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_24 (MaxPooling (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 2,118,778\n",
            "Trainable params: 2,118,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/120\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 8.5899 - acc: 0.0997 - val_loss: 2.3745 - val_acc: 0.0980\n",
            "Epoch 2/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 2.3614 - acc: 0.0970 - val_loss: 2.3520 - val_acc: 0.0952\n",
            "Epoch 3/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 2.0147 - acc: 0.2634 - val_loss: 1.6400 - val_acc: 0.4108\n",
            "Epoch 4/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 1.5871 - acc: 0.4361 - val_loss: 1.4761 - val_acc: 0.4852\n",
            "Epoch 5/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 1.3908 - acc: 0.5112 - val_loss: 1.2544 - val_acc: 0.5665\n",
            "Epoch 6/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 1.2491 - acc: 0.5717 - val_loss: 1.3039 - val_acc: 0.5705\n",
            "Epoch 7/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 1.1532 - acc: 0.6089 - val_loss: 1.0870 - val_acc: 0.6402\n",
            "Epoch 8/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 1.0770 - acc: 0.6386 - val_loss: 1.1285 - val_acc: 0.6403\n",
            "Epoch 9/120\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 1.0161 - acc: 0.6592 - val_loss: 0.8986 - val_acc: 0.7053\n",
            "Epoch 10/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.9575 - acc: 0.6825 - val_loss: 0.9515 - val_acc: 0.7026\n",
            "Epoch 11/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.9205 - acc: 0.6993 - val_loss: 0.8999 - val_acc: 0.7120\n",
            "Epoch 12/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.8754 - acc: 0.7110 - val_loss: 0.8610 - val_acc: 0.7246\n",
            "Epoch 13/120\n",
            "625/625 [==============================] - 33s 54ms/step - loss: 0.8417 - acc: 0.7282 - val_loss: 0.8461 - val_acc: 0.7323\n",
            "Epoch 14/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.8143 - acc: 0.7360 - val_loss: 0.8025 - val_acc: 0.7428\n",
            "Epoch 15/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.7946 - acc: 0.7435 - val_loss: 0.7669 - val_acc: 0.7607\n",
            "Epoch 16/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.7729 - acc: 0.7525 - val_loss: 0.7558 - val_acc: 0.7623\n",
            "Epoch 17/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.7453 - acc: 0.7650 - val_loss: 0.8202 - val_acc: 0.7484\n",
            "Epoch 18/120\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.7299 - acc: 0.7677 - val_loss: 0.7374 - val_acc: 0.7733\n",
            "Epoch 19/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.7191 - acc: 0.7731 - val_loss: 0.7135 - val_acc: 0.7805\n",
            "Epoch 20/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.7082 - acc: 0.7766 - val_loss: 0.7369 - val_acc: 0.7729\n",
            "Epoch 21/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.6949 - acc: 0.7826 - val_loss: 0.6618 - val_acc: 0.7984\n",
            "Epoch 22/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.6751 - acc: 0.7890 - val_loss: 0.6780 - val_acc: 0.7941\n",
            "Epoch 23/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.6657 - acc: 0.7921 - val_loss: 0.7159 - val_acc: 0.7829\n",
            "Epoch 24/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.6543 - acc: 0.7967 - val_loss: 0.6879 - val_acc: 0.7957\n",
            "Epoch 25/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.6413 - acc: 0.8017 - val_loss: 0.6625 - val_acc: 0.7996\n",
            "Epoch 26/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.6323 - acc: 0.8034 - val_loss: 0.6790 - val_acc: 0.7946\n",
            "Epoch 27/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.6274 - acc: 0.8071 - val_loss: 0.6629 - val_acc: 0.8034\n",
            "Epoch 28/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.6235 - acc: 0.8068 - val_loss: 0.6287 - val_acc: 0.8149\n",
            "Epoch 29/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.6120 - acc: 0.8140 - val_loss: 0.6740 - val_acc: 0.8015\n",
            "Epoch 30/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.6001 - acc: 0.8143 - val_loss: 0.6022 - val_acc: 0.8247\n",
            "Epoch 31/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5969 - acc: 0.8170 - val_loss: 0.6447 - val_acc: 0.8091\n",
            "Epoch 32/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5910 - acc: 0.8210 - val_loss: 0.6939 - val_acc: 0.8014\n",
            "Epoch 33/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.5834 - acc: 0.8237 - val_loss: 0.6395 - val_acc: 0.8170\n",
            "Epoch 34/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5795 - acc: 0.8270 - val_loss: 0.6053 - val_acc: 0.8250\n",
            "Epoch 35/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5708 - acc: 0.8266 - val_loss: 0.6171 - val_acc: 0.8201\n",
            "Epoch 36/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5659 - acc: 0.8272 - val_loss: 0.6319 - val_acc: 0.8190\n",
            "Epoch 37/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5629 - acc: 0.8304 - val_loss: 0.6383 - val_acc: 0.8177\n",
            "Epoch 38/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.5527 - acc: 0.8341 - val_loss: 0.6241 - val_acc: 0.8281\n",
            "Epoch 39/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.5451 - acc: 0.8374 - val_loss: 0.5926 - val_acc: 0.8257\n",
            "Epoch 40/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5401 - acc: 0.8384 - val_loss: 0.6706 - val_acc: 0.8129\n",
            "Epoch 41/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5369 - acc: 0.8392 - val_loss: 0.6667 - val_acc: 0.8092\n",
            "Epoch 42/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.5330 - acc: 0.8404 - val_loss: 0.6294 - val_acc: 0.8216\n",
            "Epoch 43/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.5261 - acc: 0.8432 - val_loss: 0.5890 - val_acc: 0.8313\n",
            "Epoch 44/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.5270 - acc: 0.8440 - val_loss: 0.5697 - val_acc: 0.8374\n",
            "Epoch 45/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.5206 - acc: 0.8468 - val_loss: 0.5965 - val_acc: 0.8309\n",
            "Epoch 46/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.5114 - acc: 0.8476 - val_loss: 0.6102 - val_acc: 0.8274\n",
            "Epoch 47/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5091 - acc: 0.8504 - val_loss: 0.6361 - val_acc: 0.8224\n",
            "Epoch 48/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5068 - acc: 0.8535 - val_loss: 0.6050 - val_acc: 0.8291\n",
            "Epoch 49/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.5036 - acc: 0.8506 - val_loss: 0.5958 - val_acc: 0.8350\n",
            "Epoch 50/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4953 - acc: 0.8527 - val_loss: 0.5880 - val_acc: 0.8352\n",
            "Epoch 51/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4917 - acc: 0.8553 - val_loss: 0.5822 - val_acc: 0.8391\n",
            "Epoch 52/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4931 - acc: 0.8545 - val_loss: 0.6106 - val_acc: 0.8295\n",
            "Epoch 53/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4862 - acc: 0.8594 - val_loss: 0.5839 - val_acc: 0.8374\n",
            "Epoch 54/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4853 - acc: 0.8582 - val_loss: 0.5737 - val_acc: 0.8426\n",
            "Epoch 55/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4752 - acc: 0.8619 - val_loss: 0.5436 - val_acc: 0.8514\n",
            "Epoch 56/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4788 - acc: 0.8595 - val_loss: 0.5915 - val_acc: 0.8383\n",
            "Epoch 57/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4716 - acc: 0.8637 - val_loss: 0.5680 - val_acc: 0.8428\n",
            "Epoch 58/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4652 - acc: 0.8656 - val_loss: 0.5725 - val_acc: 0.8458\n",
            "Epoch 59/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4668 - acc: 0.8668 - val_loss: 0.5970 - val_acc: 0.8364\n",
            "Epoch 60/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4621 - acc: 0.8660 - val_loss: 0.6015 - val_acc: 0.8382\n",
            "Epoch 61/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4672 - acc: 0.8646 - val_loss: 0.5895 - val_acc: 0.8343\n",
            "Epoch 62/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4583 - acc: 0.8698 - val_loss: 0.6044 - val_acc: 0.8333\n",
            "Epoch 63/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4523 - acc: 0.8697 - val_loss: 0.5825 - val_acc: 0.8425\n",
            "Epoch 64/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4518 - acc: 0.8700 - val_loss: 0.5593 - val_acc: 0.8448\n",
            "Epoch 65/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4526 - acc: 0.8702 - val_loss: 0.5393 - val_acc: 0.8562\n",
            "Epoch 66/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4480 - acc: 0.8726 - val_loss: 0.5797 - val_acc: 0.8455\n",
            "Epoch 67/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4446 - acc: 0.8728 - val_loss: 0.5844 - val_acc: 0.8415\n",
            "Epoch 68/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4409 - acc: 0.8763 - val_loss: 0.5909 - val_acc: 0.8395\n",
            "Epoch 69/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4469 - acc: 0.8745 - val_loss: 0.5667 - val_acc: 0.8491\n",
            "Epoch 70/120\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.4368 - acc: 0.8775 - val_loss: 0.6221 - val_acc: 0.8364\n",
            "Epoch 71/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4317 - acc: 0.8776 - val_loss: 0.5890 - val_acc: 0.8415\n",
            "Epoch 72/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4339 - acc: 0.8787 - val_loss: 0.5375 - val_acc: 0.8564\n",
            "Epoch 73/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4334 - acc: 0.8774 - val_loss: 0.5838 - val_acc: 0.8475\n",
            "Epoch 74/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4301 - acc: 0.8794 - val_loss: 0.5715 - val_acc: 0.8456\n",
            "Epoch 75/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4238 - acc: 0.8804 - val_loss: 0.5635 - val_acc: 0.8477\n",
            "Epoch 76/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4186 - acc: 0.8825 - val_loss: 0.5716 - val_acc: 0.8508\n",
            "Epoch 77/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4223 - acc: 0.8825 - val_loss: 0.5451 - val_acc: 0.8563\n",
            "Epoch 78/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4179 - acc: 0.8828 - val_loss: 0.5543 - val_acc: 0.8525\n",
            "Epoch 79/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.4171 - acc: 0.8816 - val_loss: 0.5557 - val_acc: 0.8521\n",
            "Epoch 80/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4152 - acc: 0.8835 - val_loss: 0.5464 - val_acc: 0.8549\n",
            "Epoch 81/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4177 - acc: 0.8825 - val_loss: 0.5694 - val_acc: 0.8505\n",
            "Epoch 82/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4095 - acc: 0.8881 - val_loss: 0.5720 - val_acc: 0.8479\n",
            "Epoch 83/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.4073 - acc: 0.8880 - val_loss: 0.5709 - val_acc: 0.8484\n",
            "Epoch 84/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4048 - acc: 0.8890 - val_loss: 0.5708 - val_acc: 0.8466\n",
            "Epoch 85/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4004 - acc: 0.8894 - val_loss: 0.5411 - val_acc: 0.8585\n",
            "Epoch 86/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.4010 - acc: 0.8900 - val_loss: 0.5421 - val_acc: 0.8577\n",
            "Epoch 87/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3981 - acc: 0.8894 - val_loss: 0.5620 - val_acc: 0.8520\n",
            "Epoch 88/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3940 - acc: 0.8924 - val_loss: 0.5506 - val_acc: 0.8557\n",
            "Epoch 89/120\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.3979 - acc: 0.8918 - val_loss: 0.5426 - val_acc: 0.8562\n",
            "Epoch 90/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3941 - acc: 0.8916 - val_loss: 0.5713 - val_acc: 0.8525\n",
            "Epoch 91/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3913 - acc: 0.8931 - val_loss: 0.5829 - val_acc: 0.8488\n",
            "Epoch 92/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3944 - acc: 0.8916 - val_loss: 0.5823 - val_acc: 0.8482\n",
            "Epoch 93/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3932 - acc: 0.8933 - val_loss: 0.5388 - val_acc: 0.8557\n",
            "Epoch 94/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3864 - acc: 0.8959 - val_loss: 0.5629 - val_acc: 0.8530\n",
            "Epoch 95/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3853 - acc: 0.8956 - val_loss: 0.5867 - val_acc: 0.8494\n",
            "Epoch 96/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3820 - acc: 0.8954 - val_loss: 0.5886 - val_acc: 0.8486\n",
            "Epoch 97/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3857 - acc: 0.8949 - val_loss: 0.5530 - val_acc: 0.8558\n",
            "Epoch 98/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3795 - acc: 0.8976 - val_loss: 0.5671 - val_acc: 0.8532\n",
            "Epoch 99/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3829 - acc: 0.8973 - val_loss: 0.5667 - val_acc: 0.8522\n",
            "Epoch 100/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3816 - acc: 0.8966 - val_loss: 0.5588 - val_acc: 0.8526\n",
            "Epoch 101/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3751 - acc: 0.8987 - val_loss: 0.5616 - val_acc: 0.8581\n",
            "Epoch 102/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3702 - acc: 0.9009 - val_loss: 0.5514 - val_acc: 0.8605\n",
            "Epoch 103/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.3702 - acc: 0.9009 - val_loss: 0.5377 - val_acc: 0.8653\n",
            "Epoch 104/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3763 - acc: 0.8998 - val_loss: 0.5481 - val_acc: 0.8582\n",
            "Epoch 105/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3706 - acc: 0.9024 - val_loss: 0.5803 - val_acc: 0.8518\n",
            "Epoch 106/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3699 - acc: 0.9002 - val_loss: 0.6112 - val_acc: 0.8484\n",
            "Epoch 107/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3683 - acc: 0.9018 - val_loss: 0.5405 - val_acc: 0.8588\n",
            "Epoch 108/120\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3671 - acc: 0.9022 - val_loss: 0.5543 - val_acc: 0.8586\n",
            "Epoch 109/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3634 - acc: 0.9035 - val_loss: 0.5598 - val_acc: 0.8543\n",
            "Epoch 110/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.3650 - acc: 0.9012 - val_loss: 0.5482 - val_acc: 0.8563\n",
            "Epoch 111/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.3646 - acc: 0.9030 - val_loss: 0.5357 - val_acc: 0.8599\n",
            "Epoch 112/120\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.3580 - acc: 0.9053 - val_loss: 0.5888 - val_acc: 0.8524\n",
            "Epoch 113/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3608 - acc: 0.9049 - val_loss: 0.5583 - val_acc: 0.8596\n",
            "Epoch 114/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3581 - acc: 0.9051 - val_loss: 0.5463 - val_acc: 0.8594\n",
            "Epoch 115/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3506 - acc: 0.9078 - val_loss: 0.5594 - val_acc: 0.8575\n",
            "Epoch 116/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3588 - acc: 0.9060 - val_loss: 0.5666 - val_acc: 0.8559\n",
            "Epoch 117/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3581 - acc: 0.9041 - val_loss: 0.5473 - val_acc: 0.8589\n",
            "Epoch 118/120\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.3550 - acc: 0.9068 - val_loss: 0.5743 - val_acc: 0.8558\n",
            "Epoch 119/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3531 - acc: 0.9076 - val_loss: 0.5544 - val_acc: 0.8591\n",
            "Epoch 120/120\n",
            "625/625 [==============================] - 31s 50ms/step - loss: 0.3501 - acc: 0.9082 - val_loss: 0.5525 - val_acc: 0.8610\n",
            "Saved model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bnYzxgc7-bEO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**In the penultimate step, We evaluate our trained models on the test dataset. We choose the best architecture out of the four architectures. **"
      ]
    },
    {
      "metadata": {
        "id": "lSXLfSQ8-JTr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "3f6216b6-951d-42cf-f782-789e55306a6e"
      },
      "cell_type": "code",
      "source": [
        "######  General CNN without dropout #######\n",
        "\n",
        "print('General CNN without dropout')\n",
        "# Load json and create model\n",
        "json_file = open(\"model_without_dropout.json\", 'r')            \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model_0 = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into new model\n",
        "loaded_model_0.load_weights(model_path_1)                                          \n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "## Score the loaded model on the test dataset\n",
        "loaded_model_0.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "scores = loaded_model_0.evaluate(X_test, y_test, verbose=1)\n",
        "print('Loss:', scores[0])\n",
        "print('Accuracy of CNN without dropout on test data:', scores[1])\n",
        "\n",
        "\n",
        "######  General CNN with dropout #######\n",
        "\n",
        "print('General CNN with dropout')\n",
        "# Load json and create model\n",
        "json_file = open(\"model_with_dropout.json\", 'r')            \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into new model\n",
        "loaded_model.load_weights(model_path_2)                                          \n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "## Score the loaded model on the test dataset\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "scores = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Loss:', scores[0])\n",
        "print('Accuracy of CNN with dropout on test data:', scores[1])\n",
        "\n",
        "\n",
        "######  For Data Augmentation with dropout #######\n",
        "\n",
        "print('For Data Augmentation with dropout')\n",
        "# Load json and create model\n",
        "json_file = open(\"model_Data_augmentation_with_dropout.json\", 'r')            \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model_1 = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into new model\n",
        "loaded_model_1.load_weights(model_path_3)                                          \n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "## Score the loaded model on the test dataset\n",
        "loaded_model_1.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "scores = loaded_model_1.evaluate(X_test, y_test, verbose=1)\n",
        "print('Loss:', scores[0])\n",
        "print('Accuracy of model with data augmentation and dropout on test data:', scores[1])\n",
        "\n",
        "\n",
        "######  For Data Augmentation with No dropout #######\n",
        "\n",
        "print('For Data Augmentation without the dropout')\n",
        "# Load json and create model\n",
        "json_file = open(\"model_Data_augmentation_without_dropout.json\", 'r')            \n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model_2 = model_from_json(loaded_model_json)\n",
        "\n",
        "# Load weights into new model\n",
        "loaded_model_2.load_weights(model_path_4)                                          \n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "## Score the loaded model on the test dataset\n",
        "loaded_model_2.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "scores = loaded_model_2.evaluate(X_test, y_test, verbose=1)\n",
        "print('Loss:', scores[0])\n",
        "print('Accuracy of model with data augmentation and NO dropout on test data:', scores[1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "General CNN without dropout\n",
            "Loaded model from disk\n",
            "10000/10000 [==============================] - 9s 914us/step\n",
            "Loss: 1.7323800757408143\n",
            "Accuracy of CNN without dropout on test data: 0.7592\n",
            "General CNN with dropout\n",
            "Loaded model from disk\n",
            "10000/10000 [==============================] - 9s 919us/step\n",
            "Loss: 0.6480929811477661\n",
            "Accuracy of CNN with dropout on test data: 0.8415\n",
            "For Data Augmentation with dropout\n",
            "Loaded model from disk\n",
            "10000/10000 [==============================] - 9s 946us/step\n",
            "Loss: 0.6004882764339448\n",
            "Accuracy of model with data augmentation and dropout on test data: 0.8326\n",
            "For Data Augmentation without the dropout\n",
            "Loaded model from disk\n",
            "10000/10000 [==============================] - 9s 937us/step\n",
            "Loss: 0.5773086317539216\n",
            "Accuracy of model with data augmentation and NO dropout on test data: 0.8525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fBzcUF49_Hy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Finally, We do the K-fold cross-validation of the best model.**"
      ]
    },
    {
      "metadata": {
        "id": "TnsdZXht_OnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2145
        },
        "outputId": "e405ef9e-7c3f-4e98-a8dc-a746ac88a9a8"
      },
      "cell_type": "code",
      "source": [
        "## Get the training dataset again\n",
        "X_train = train_images_1.reshape(train_images_1.shape[0], 32, 32, 3)\n",
        "y_train_Kfold = np_utils.to_categorical(train_labels_1, nb_classes)\n",
        "X_train_Kfold = (X_train/255).astype('float32')\n",
        "\n",
        "\n",
        "cvscores = []\n",
        "cv = KFold(n_splits=5, random_state=42, shuffle=False)     ## we set K=5\n",
        "for train_index, test_index in cv.split(X_train_Kfold):\n",
        "  \n",
        "    ## K-fold Split of the dataset(where K=5, 4 of the folds are for training and 1 of them is for validating)\n",
        "    X_train, X_val, y_train, y_val = X_train_Kfold[train_index], X_train_Kfold[test_index], y_train_Kfold[train_index], y_train_Kfold[test_index]\n",
        "\n",
        "    ## Compile the optimizer and train \n",
        "    print(\"New fold:\")\n",
        "    loaded_model_2.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "    loaded_model_2.fit(X_train, y_train, epochs=10, batch_size=40)\n",
        "\n",
        "    ## Get the score on the test dataset\n",
        "    scores = loaded_model_2.evaluate(X_test, y_test)\n",
        "    print(\"%s: %.2f%%\" % (loaded_model_2.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "    \n",
        "print(\"K-fold cross validation is DONE...\")\n",
        "print(\"Cross-validation Accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New fold:\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 37s 932us/step - loss: 0.7963 - acc: 0.7353\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 31s 772us/step - loss: 0.5661 - acc: 0.8079\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 31s 767us/step - loss: 0.5080 - acc: 0.8312\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.4684 - acc: 0.8460\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.4354 - acc: 0.8582\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.4102 - acc: 0.8673\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.3878 - acc: 0.8756\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.3671 - acc: 0.8837\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 31s 767us/step - loss: 0.3475 - acc: 0.8902\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.3328 - acc: 0.8949\n",
            "10000/10000 [==============================] - 6s 559us/step\n",
            "acc: 79.50%\n",
            "New fold:\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 37s 929us/step - loss: 1.0451 - acc: 0.7699\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 31s 769us/step - loss: 0.4211 - acc: 0.8653\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.3688 - acc: 0.8833\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 31s 769us/step - loss: 0.3311 - acc: 0.8990\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 31s 771us/step - loss: 0.3035 - acc: 0.9090\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 31s 771us/step - loss: 0.2797 - acc: 0.9180\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.2593 - acc: 0.9262\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.2412 - acc: 0.9323\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.2257 - acc: 0.9389\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 31s 765us/step - loss: 0.2111 - acc: 0.9452\n",
            "10000/10000 [==============================] - 6s 574us/step\n",
            "acc: 79.76%\n",
            "New fold:\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 37s 929us/step - loss: 14.5204 - acc: 0.1007\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 5.4481 - acc: 0.5282\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.4466 - acc: 0.8519\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.3662 - acc: 0.8842\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.3179 - acc: 0.8996\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.2840 - acc: 0.9146\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 31s 767us/step - loss: 0.2571 - acc: 0.9247\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.2337 - acc: 0.9326\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 31s 765us/step - loss: 0.2131 - acc: 0.9413\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 31s 764us/step - loss: 0.1964 - acc: 0.9486\n",
            "10000/10000 [==============================] - 6s 579us/step\n",
            "acc: 79.79%\n",
            "New fold:\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 37s 930us/step - loss: 1.1057 - acc: 0.7895\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 31s 769us/step - loss: 0.3158 - acc: 0.9019\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.2550 - acc: 0.9256\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 31s 770us/step - loss: 0.2182 - acc: 0.9379\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.1901 - acc: 0.9497\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.1673 - acc: 0.9592\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 31s 765us/step - loss: 0.1489 - acc: 0.9664\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.1344 - acc: 0.9717\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 31s 767us/step - loss: 0.1205 - acc: 0.9768\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.1092 - acc: 0.9819\n",
            "10000/10000 [==============================] - 6s 581us/step\n",
            "acc: 79.51%\n",
            "New fold:\n",
            "Epoch 1/10\n",
            "40000/40000 [==============================] - 37s 931us/step - loss: 10.2290 - acc: 0.2761\n",
            "Epoch 2/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.5079 - acc: 0.8295\n",
            "Epoch 3/10\n",
            "40000/40000 [==============================] - 31s 768us/step - loss: 0.3741 - acc: 0.8788\n",
            "Epoch 4/10\n",
            "40000/40000 [==============================] - 31s 771us/step - loss: 0.3115 - acc: 0.9016\n",
            "Epoch 5/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.2679 - acc: 0.9187\n",
            "Epoch 6/10\n",
            "40000/40000 [==============================] - 31s 766us/step - loss: 0.2348 - acc: 0.9304\n",
            "Epoch 7/10\n",
            "40000/40000 [==============================] - 31s 767us/step - loss: 0.2073 - acc: 0.9409\n",
            "Epoch 8/10\n",
            "40000/40000 [==============================] - 31s 765us/step - loss: 0.1847 - acc: 0.9505\n",
            "Epoch 9/10\n",
            "40000/40000 [==============================] - 31s 764us/step - loss: 0.1678 - acc: 0.9568\n",
            "Epoch 10/10\n",
            "40000/40000 [==============================] - 31s 765us/step - loss: 0.1493 - acc: 0.9662\n",
            "10000/10000 [==============================] - 6s 575us/step\n",
            "acc: 79.54%\n",
            "K-fold cross validation is DONE...\n",
            "Cross-validation Accuracy: 79.62% (+/- 0.13%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}